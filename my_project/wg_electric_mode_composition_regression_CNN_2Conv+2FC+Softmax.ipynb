{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import linecache\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize mode params\n",
    "num_mode=3\n",
    "ny=161\n",
    "nz=23\n",
    "y=np.zeros((ny,1))\n",
    "z=np.zeros((nz,1))\n",
    "data1=np.zeros((ny,nz))\n",
    "data2=np.zeros((ny,nz))\n",
    "mode_data=[['' for i in range(3)] for j in range(1)]\n",
    "\n",
    "## file directory matching diff system distribution\n",
    "file_str = '.*Mode_data*mode'\n",
    "file_str_list = list(file_str)\n",
    "index = 0\n",
    "for cha in file_str_list:\n",
    "    if cha == '*':\n",
    "        file_str_list[index] = os.sep\n",
    "    index += 1\n",
    "file_str = ''.join(file_str_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mode1-3 E and H data\n",
    "j=0\n",
    "for i in range(3+1,3+1+ny):\n",
    "    y[j,:]=linecache.getline(file_str+'1_ey.txt', i).split()\n",
    "    j+=1\n",
    "\n",
    "j=0\n",
    "for i in range(3+ny+2+1,3+ny+2+1+nz):\n",
    "    z[j,:]=linecache.getline(file_str+'1_ey.txt', i).split()\n",
    "    j+=1\n",
    "\n",
    "for k in range(num_mode):\n",
    "    j=0\n",
    "    for i in range(3+ny+2+nz+2+1,3+ny+2+nz+2+1+ny):\n",
    "        data1[j,:]=linecache.getline(file_str+str(k+1)+'_ey.txt', i).split()\n",
    "        j+=1  \n",
    "    mode_data[0][k]=np.copy(data1.T)\n",
    "\n",
    "# Normlize integral{|E|^2*dydz} to 1\n",
    "dy=(y[-1,0]-y[0,0])/(y.shape[0]-1)\n",
    "dz=(z[-1,0]-z[0,0])/(z.shape[0]-1)\n",
    "norm_mode=np.zeros((1,num_mode))\n",
    "for i in range(num_mode):\n",
    "    norm_mode[0,i]=np.sum(mode_data[0][i]**2*dy*dz)\n",
    "    mode_data[0][i]/=np.sqrt(norm_mode[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show three Ey modes\n",
    "# generate 2d grids for y and z\n",
    "yy,zz=np.meshgrid(y,z)\n",
    "plt.figure()\n",
    "for i in range(num_mode):\n",
    "    plt.subplot(num_mode, 1, i+1)\n",
    "    plt.title('mode{}_ey'.format(i+1))\n",
    "    plt.pcolormesh(yy,zz,mode_data[0][i])\n",
    "    plt.xlabel(\"Y direction/micron\")\n",
    "    plt.ylabel(\"Z direction/micron\")\n",
    "    plt.ylim(-0.11, 0.33)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesis data set\n",
    "\n",
    "m_base=1000\n",
    "\n",
    "if False:\n",
    "    m_all=m_base # number of all the samples\n",
    "\n",
    "    # pert=0.05 # perturbation of intensity after field's superposition\n",
    "\n",
    "    np.random.seed(0) # random seed(0)\n",
    "    rd1=np.random.randn(3,m_base)\n",
    "    np.random.seed(1) # random seed(1)\n",
    "    rd2=np.random.rand(3,m_base)\n",
    "\n",
    "    mode_comp=np.zeros((num_mode,m_all),dtype=complex)\n",
    "    mode_comp[:,:m_base]=np.abs(rd1)*np.exp(1j*rd2*2*np.pi)\n",
    "else:\n",
    "    f = open('.'+os.sep+'training_data_add'+os.sep+'training_data_add_1_111317', 'rb')\n",
    "    training_data_add_1 = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    m_all=m_base+len(training_data_add_1) # number of all the samples\n",
    "\n",
    "    # pert=0.05 # perturbation of intensity after field's superposition\n",
    "\n",
    "    np.random.seed(0) # random seed(0)\n",
    "    rd1=np.random.randn(3,m_base)\n",
    "    np.random.seed(1) # random seed(1)\n",
    "    rd2=np.random.rand(3,m_base)\n",
    "\n",
    "    mode_comp=np.zeros((num_mode,m_all),dtype=complex)\n",
    "    mode_comp[:,:m_base]=np.abs(rd1)*np.exp(1j*rd2*2*np.pi)\n",
    "    mode_comp[:,m_base:m_base+len(training_data_add_1)]=np.array(training_data_add_1).T\n",
    "    \n",
    "field_sp=[[0+1j for i in range(m_all)] for j in range(1)] # [[Ey(0),...,Ey(m_all-1)]]\n",
    "mode_comp_label=np.zeros((num_mode,m_all))\n",
    "for i in range(m_all):\n",
    "    for j in range(num_mode):\n",
    "        field_sp[0][i]+=mode_comp[j,i]*mode_data[0][j]\n",
    "mode_comp_label=np.abs(mode_comp)**2/np.sum(np.abs(mode_comp)**2,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_comp_label[:,132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view specified sample\n",
    "index=132\n",
    "plt.figure(num=None, figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title('index_{}_|ey|'.format(index))\n",
    "plt.pcolormesh(yy,zz,np.abs(field_sp[0][index])/np.max(np.abs(field_sp[0][index])))\n",
    "plt.xlabel(\"Y direction/micron\")\n",
    "plt.ylabel(\"Z direction/micron\")\n",
    "plt.ylim(-0.11, 0.33)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.colorbar(orientation='horizontal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get farfield diffraction pattern\n",
    "sp_y=y.shape[0] # sampling points along y\n",
    "sp_z=z.shape[0] # sampling points along z\n",
    "ff_data=[]\n",
    "for i in range(m_all):\n",
    "    ff_data.append(np.fft.fftshift(np.fft.fft2(field_sp[0][i]))/sp_y/sp_z)\n",
    "\n",
    "ff_i_data=['' for i in range(m_all)]\n",
    "for i in range(m_all):\n",
    "    ff_i_data[i]=np.abs(ff_data[i])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view specified farfield pattern\n",
    "sl_y=dy # sampling length(unit) along y\n",
    "sl_z=dz # sampling length(unit) along z\n",
    "sf_y=1/dy # sampling frequency along y\n",
    "sf_z=1/dz # sampling frequency along z\n",
    "f_y=np.fft.fftshift(np.fft.fftfreq(sp_y,sl_y))\n",
    "f_z=np.fft.fftshift(np.fft.fftfreq(sp_z,sl_z))\n",
    "index=132\n",
    "plt.figure(num=None, figsize=(5,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.pcolormesh(f_y,f_z,10*np.log10(ff_i_data[index]/np.max(ff_i_data[index])))\n",
    "plt.title('Far field intensity distribution/dB')\n",
    "plt.xlabel('far field y/a.u.')\n",
    "plt.ylabel('far field z/a.u.')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_i_data[index].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_index=np.random.randint(m_all)\n",
    "ff_i_data_eff_index_0,ff_i_data_eff_index_1=np.nonzero((ff_i_data[temp_index]/np.max(ff_i_data[temp_index]))>=1e-3)\n",
    "print(ff_i_data_eff_index_0)\n",
    "print(np.min(ff_i_data_eff_index_0),np.max(ff_i_data_eff_index_0))\n",
    "print(ff_i_data_eff_index_1)\n",
    "print(np.min(ff_i_data_eff_index_1),np.max(ff_i_data_eff_index_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_i_data_eff=['' for i in range(m_all)]\n",
    "for i in range(m_all):\n",
    "    ff_i_data_eff[i]=ff_i_data[i][9:12,70:91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view specified farfield pattern\n",
    "index=132\n",
    "plt.figure(num=None, figsize=(5,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.imshow(-np.log10(ff_i_data_eff[index]))\n",
    "plt.title('Far field intensity distribution/dB')\n",
    "plt.xlabel('far field y/a.u.')\n",
    "plt.ylabel('far field z/a.u.')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(None)\n",
    "permutation = list(np.random.permutation(m_all))\n",
    "ff_i_data_eff_shuffled=np.array(ff_i_data_eff)[permutation,:,:]\n",
    "ff_i_data_eff_shuffled=ff_i_data_eff_shuffled.reshape(m_all,ff_i_data_eff_shuffled.shape[1],\n",
    "                                                      ff_i_data_eff_shuffled.shape[2],1)\n",
    "mode_comp_label_shuffled=mode_comp_label[:,permutation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_comp_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = -np.log10(ff_i_data_eff_shuffled[:900,:,:,:])\n",
    "Y_train = mode_comp_label_shuffled[:,:900].T\n",
    "X_test = -np.log10(ff_i_data_eff_shuffled[900:,:,:,:])\n",
    "Y_test = mode_comp_label_shuffled[:,900:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(learning_rate):\n",
    "\n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=[None, X_train.shape[1],X_train.shape[2],1])\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, Y_train.shape[-1]])\n",
    "    \n",
    "    with tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([3, 3, 1, 16]) # [filter_height, filter_width, in_channels, out_channels]\n",
    "        b_conv1 = bias_variable([16])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1) # [batch, 3, 21, 16]\n",
    "        h_pool1 = max_pool_2x2(h_conv1) # [batch, 2, 11, 16]\n",
    "    \n",
    "    with tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([3, 3, 16, 32])\n",
    "        b_conv2 = bias_variable([32])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # [batch, 2, 11, 32]\n",
    "        h_pool2 = max_pool_2x2(h_conv2) # [batch, 1, 6, 32]\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, h_pool2.shape[1]*h_pool2.shape[2]*h_pool2.shape[3]])\n",
    "    \n",
    "    with tf.name_scope('fc1'):\n",
    "        W_fc1 = weight_variable([np.int(h_pool2_flat.shape[-1]), 64])\n",
    "        b_fc1 = bias_variable([64])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "        # dropout\n",
    "        keep_prob_1 = tf.placeholder(tf.float32)\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob_1)\n",
    "        \n",
    "    with tf.name_scope('fc2'):\n",
    "        W_fc2 = weight_variable([64, 32])\n",
    "        b_fc2 = bias_variable([32])\n",
    "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "        # dropout\n",
    "        keep_prob_2 = tf.placeholder(tf.float32)\n",
    "        h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob_2)\n",
    "        \n",
    "    with tf.name_scope('fc3'):\n",
    "        W_fc3 = weight_variable([32, 3])\n",
    "        b_fc3 = bias_variable([3])\n",
    "        y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3\n",
    "        y = tf.nn.softmax(y_conv)\n",
    "    cost = tf.reduce_mean(tf.reduce_sum((y-y_)**2,axis=-1))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    return keep_prob_1, keep_prob_2, x, y_, y, cost, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(None)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 1e-4,\n",
    "          num_epochs = 1500, minibatch_size = 32, keep_prob_1_value = 1, keep_prob_2_value = 1, print_cost = True, index = 0):\n",
    "    \n",
    "    m = X_train.shape[0]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    keep_prob_1, keep_prob_2, x, y_, y, cost, optimizer = build_graph(learning_rate)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    time_cost = []\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        # summary_writer = tf.summary.FileWriter('.'+os.sep+'mc_regr_cnn'+os.sep+'graph', sess.graph)\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            if epoch % 100 ==0:\n",
    "                time_cost.append(time.time())\n",
    "                if epoch != 0:\n",
    "                    time_cost_average = (time_cost[-1]-time_cost[0])/(len(time_cost)-1)/100*(num_epochs-epoch)\n",
    "                    print(\"The simulation will be done at: \", time.ctime(time_cost_average+time.time()))\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={keep_prob_1: keep_prob_1_value, keep_prob_2: keep_prob_2_value, \n",
    "                                                                            x: minibatch_X, y_: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "                \n",
    "            if epoch == 0:\n",
    "                test_cost_min = sess.run(cost,feed_dict={keep_prob_1: 1, keep_prob_2: 1, x: X_test, y_: Y_test})\n",
    "            if epoch % 100 ==0:\n",
    "                train_cost = sess.run(cost,feed_dict={keep_prob_1: 1, keep_prob_2: 1, x: X_train, y_: Y_train})\n",
    "                test_cost = sess.run(cost,feed_dict={keep_prob_1: 1, keep_prob_2: 1, x: X_test, y_: Y_test})\n",
    "                if test_cost<test_cost_min:\n",
    "                    test_cost_min=test_cost\n",
    "                    saved_min_tc_model = saver.save(sess, '.'+os.sep+'mc_regr_cnn'+os.sep+\n",
    "                                                    'model_2C2F1S'+os.sep+'model_min_tc_'+np.str(index))\n",
    "                if print_cost == True:\n",
    "                    print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "                    print (\"Train mean square error:\", train_cost)\n",
    "                    print (\"Test mean square error:\", test_cost)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                costs.append(epoch_cost)  \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "        saved_model = saver.save(sess, '.'+os.sep+'mc_regr_cnn'+os.sep+'model_2C2F1S'+os.sep+'model_'+np.str(index))\n",
    "        print(\"Trained model has been saved.\")\n",
    "        print (\"Train mean square error:\", sess.run(cost,feed_dict={keep_prob_1: 1, keep_prob_2: 1, x: X_train, y_: Y_train}))\n",
    "        print (\"Test mean square error:\", sess.run(cost,feed_dict={keep_prob_1: 1, keep_prob_2: 1, x: X_test, y_: Y_test}))\n",
    "        return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index: 0, learning_rate = 1e-4, num_epochs = 20000, minibatch_size = 16, keep_prob_1_value=0.9, keep_prob_2_value=0.9\n",
    "# index: 1, learning_rate = 1e-4, num_epochs = 20000, minibatch_size = 16, keep_prob_1_value=1, keep_prob_2_value=1\n",
    "# index: 2, learning_rate = 1e-4, num_epochs = 20000, minibatch_size = 16, keep_prob_1_value=0.9, keep_prob_2_value=1\n",
    "# training_data_add_1_111317, m_all = 1190\n",
    "# index: 3, learning_rate = 1e-4, num_epochs = 20000, minibatch_size = 16, keep_prob_1_value=1, keep_prob_2_value=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = model(X_train, Y_train, X_test, Y_test, learning_rate = 1e-4,\n",
    "            num_epochs = 20000, minibatch_size = 16, keep_prob_1_value = 1, keep_prob_2_value = 1, print_cost = True, index = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "keep_prob_1, keep_prob_2, x, y_, y, cost, optimizer = build_graph(learning_rate=1e-4)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, '.'+os.sep+'mc_regr_cnn'+os.sep+'model_2C2F1S'+os.sep+'model_min_tc_1')\n",
    "    print (\"Train mean square error:\", sess.run(cost,feed_dict={keep_prob_1: 1, keep_prob_2: 1, x: X_train, y_: Y_train}))\n",
    "    print (\"Test mean square error:\", sess.run(cost,feed_dict={keep_prob_1: 1, keep_prob_2: 1, x: X_test, y_: Y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
