{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import linecache\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize mode params\n",
    "num_mode=3\n",
    "ny=161\n",
    "nz=23\n",
    "y=np.zeros((ny,1))\n",
    "z=np.zeros((nz,1))\n",
    "data1=np.zeros((ny,nz))\n",
    "data2=np.zeros((ny,nz))\n",
    "mode_data=[['' for i in range(3)] for j in range(2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get mode1-3 E and H data\n",
    "j=0\n",
    "for i in range(3+1,3+1+ny):\n",
    "    y[j,:]=linecache.getline(r'C:\\Users\\Ang Liu\\Desktop\\Mode_data\\mode1_ey.txt', i).split()\n",
    "    j+=1\n",
    "\n",
    "j=0\n",
    "for i in range(3+ny+2+1,3+ny+2+1+nz):\n",
    "    z[j,:]=linecache.getline(r'C:\\Users\\Ang Liu\\Desktop\\Mode_data\\mode1_ey.txt', i).split()\n",
    "    j+=1\n",
    "\n",
    "for k in range(num_mode):\n",
    "    j=0\n",
    "    for i in range(3+ny+2+nz+2+1,3+ny+2+nz+2+1+ny):\n",
    "        data1[j,:]=linecache.getline(r'C:\\Users\\Ang Liu\\Desktop\\Mode_data\\mode'+str(k+1)+'_ey.txt', i).split()\n",
    "        data2[j,:]=linecache.getline(r'C:\\Users\\Ang Liu\\Desktop\\Mode_data\\mode'+str(k+1)+'_hz.txt', i).split()\n",
    "        j+=1  \n",
    "    mode_data[0][k]=np.copy(data1.T)\n",
    "    mode_data[1][k]=np.copy(data2.T)\n",
    "\n",
    "# Normlize mode energy to 1\n",
    "dy=(y[-1,0]-y[0,0])/(y.shape[0]-1)\n",
    "dz=(z[-1,0]-z[0,0])/(z.shape[0]-1)\n",
    "norm_mode=np.zeros((1,num_mode))\n",
    "for i in range(num_mode):\n",
    "    norm_mode[0,i]=np.sum(mode_data[0][i]*mode_data[1][i]*dy*dz)/2\n",
    "    mode_data[0][i]/=np.sqrt(norm_mode[0,i])\n",
    "    mode_data[1][i]/=np.sqrt(norm_mode[0,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show three modes (Ey and Hz)\n",
    "# generate 2d grids for y and z\n",
    "yy,zz=np.meshgrid(y,z)\n",
    "plt.figure()\n",
    "for i in range(num_mode):\n",
    "    plt.subplot(num_mode, 2, 2*i+1)\n",
    "    plt.title('mode{}_ey'.format(i+1))\n",
    "    plt.pcolormesh(yy,zz,mode_data[0][i])\n",
    "    plt.subplot(num_mode, 2, 2*i+2)\n",
    "    plt.title('mode{}_hz'.format(i+1))\n",
    "    plt.pcolormesh(yy,zz,mode_data[1][i])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Synthesis data set\n",
    "m_all=300 # number of all the samples\n",
    "pert=0.05 # perturbation of intensity after field's superposition\n",
    "field_sp=[[0 for i in range(m_all)] for j in range(2)] # [[Ey(0),...,Ey(m_all-1)],[Hz(0),...,Hz(m_all-1)]]\n",
    "np.random.seed(0) # random seed(0)\n",
    "rd1=np.random.rand(3,m_all)\n",
    "np.random.seed(1) # random seed(1)\n",
    "rd2=np.random.rand(3,m_all)\n",
    "mode_comp=rd1*np.exp(1j*rd2*2*np.pi)\n",
    "mode_comp_label=rd1**2/np.sum(rd1**2,axis=0)\n",
    "for i in range(m_all):\n",
    "    for j in range(num_mode):\n",
    "        field_sp[0][i]+=mode_comp[j,i]*mode_data[0][j]\n",
    "        field_sp[1][i]+=mode_comp[j,i]*mode_data[1][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view specified sample\n",
    "index=199\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('mode{}_ey'.format(index))\n",
    "plt.pcolormesh(yy,zz,abs(field_sp[0][index]))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('mode{}_hz'.format(index))\n",
    "plt.pcolormesh(yy,zz,abs(field_sp[1][index]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get farfield diffraction pattern\n",
    "sp_y=y.shape[0] # sampling points along y\n",
    "sp_z=z.shape[0] # sampling points along z\n",
    "ff_data=[]\n",
    "for i in range(m_all):\n",
    "    ff_data.append(np.fft.fftshift(np.fft.fft2(field_sp[0][i]))/sp_y/sp_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# view specified farfield pattern\n",
    "sl_y=dy # sampling length(unit) along y\n",
    "sl_z=dz # sampling length(unit) along z\n",
    "sf_y=1/dy # sampling frequency along y\n",
    "sf_z=1/dz # sampling frequency along z\n",
    "f_y=np.fft.fftshift(np.fft.fftfreq(sp_y,sl_y))\n",
    "f_z=np.fft.fftshift(np.fft.fftfreq(sp_z,sl_z))\n",
    "index=80\n",
    "plt.pcolormesh(f_y,f_z,(np.log2(np.abs(ff_data[index]))**2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=np.abs(ff_data[:200])**2\n",
    "X_train=X_train.reshape(200,-1).T\n",
    "X_test=np.abs(ff_data[200:300])**2\n",
    "X_test=X_test.reshape(100,-1).T\n",
    "Y_train=mode_comp_label[:,:200]\n",
    "Y_test=mode_comp_label[:,200:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32,[n_x,None])\n",
    "    Y = tf.placeholder(tf.float32,[n_y,None])\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    tf.set_random_seed(2)      \n",
    "    W1 = tf.get_variable(\"W1\", [25,3703], initializer = tf.contrib.layers.xavier_initializer(seed = 3))\n",
    "    b1 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 4))\n",
    "    b2 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [3,12], initializer = tf.contrib.layers.xavier_initializer(seed = 5))\n",
    "    b3 = tf.get_variable(\"b3\", [3,1], initializer = tf.zeros_initializer())\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):   \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.matmul(W1,X) + b1                                              # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.tanh(Z1)                                              # A1 = tanh(Z1)\n",
    "    Z2 = tf.matmul(W2,A1) + b2                                              # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.tanh(Z2)                                              # A2 = tanh(Z2)\n",
    "    Z3 = tf.matmul(W3,A2) + b3                                              # Z3 = np.dot(W3,Z2) + b3\n",
    "   \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    Z3 = tf.transpose(Z3)\n",
    "    Y = tf.transpose(Y)\n",
    "    A3 = tf.nn.softmax(Z3)\n",
    "    cost = tf.reduce_mean(tf.reduce_sum((Y-A3)**2,axis=1))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    seed = 6                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z3 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    \n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        print (\"Train perturbation:\", sess.run(cost,feed_dict={X: X_train, Y: Y_train}))\n",
    "        print (\"Test perturbation:\", sess.run(cost,feed_dict={X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test,minibatch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test,minibatch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test,minibatch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
